{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d13b72f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce MX450\n",
      "CUDA version: 12.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45594"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"  # force cpu\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Load words\n",
    "words = []\n",
    "# with open(\"./data/western_names.txt\") as file:\n",
    "#     words = file.read().splitlines()\n",
    "\n",
    "with open(\"./data/indian_names.csv\", encoding=\"utf-8\") as file:\n",
    "    df = pd.read_csv(file, header=None)\n",
    "    words = df.iloc[:, 0].dropna().tolist()\n",
    "    words = [w.lower() for w in words if w.isalpha()]\n",
    "    words = words[1:]\n",
    "words[0:10]\n",
    "min(len(w) for w in words), max(len(w) for w in words)\n",
    "len(\"\".join(w for w in words)) + len(\n",
    "    words\n",
    ")  # total chars from all words +len(words) for the dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "66a3d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize character embeddings\n",
    "unique_tokens = sorted(list(set(\"\".join(words))))\n",
    "\n",
    "token_to_idx = {ch: i + 1 for i, ch in enumerate(unique_tokens)}\n",
    "token_to_idx[\".\"] = 0\n",
    "idx_to_token = {i: ch for ch, i in token_to_idx.items()}\n",
    "vocab_size = len(token_to_idx)\n",
    "eps = 1e-5\n",
    "\n",
    "# hyperparameters on cpu\n",
    "# context_size = 4\n",
    "# embed_size = 8\n",
    "# batch_size = 32\n",
    "# network_size = 128\n",
    "\n",
    "# hyperparameters on gpu\n",
    "context_size = 8\n",
    "embed_size = 64\n",
    "batch_size = 2048\n",
    "network_size = 1024\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * context_size\n",
    "        for ch in w + \".\":\n",
    "            idx = token_to_idx[ch]\n",
    "            X.append(context)\n",
    "            Y.append(idx)\n",
    "            context = context[1:] + [idx]\n",
    "    X = torch.tensor(X, device=device)\n",
    "    Y = torch.tensor(Y, device=device)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "X_train, Y_train = build_dataset(words[:n1])\n",
    "X_val, Y_val = build_dataset(words[n1:n2])\n",
    "X_test, Y_test = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a002db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True, generator=None, device=None):\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=generator, device=device) / fan_in**0.5\n",
    "        self.bias = torch.zeros(1, fan_out, device=device) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias] if self.bias is not None else [self.weight]\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1, device=None):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.dim = dim\n",
    "        self.gamma = torch.ones(dim, device=device)\n",
    "        self.beta = torch.zeros(dim, device=device)\n",
    "        self.running_mean = torch.zeros(dim, device=device)\n",
    "        self.running_var = torch.ones(dim, device=device)\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            x_mean = x.mean(0, keepdim=True)\n",
    "            x_var = x.var(0, keepdim=True, unbiased=False)\n",
    "        else:\n",
    "            x_mean = self.running_mean\n",
    "            x_var = self.running_var\n",
    "\n",
    "        x_hat = (x - x_mean) / torch.sqrt(x_var + self.eps)\n",
    "        self.out = (self.gamma * x_hat) + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * x_mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * x_var\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "16b99b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:  4759259\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "\n",
    "E = torch.randn((vocab_size, embed_size), generator=g, device=device)\n",
    "\n",
    "Layers = [\n",
    "    Linear(context_size * embed_size, network_size, bias=True, generator=g, device=device), BatchNorm1d(network_size, device=device), Tanh(),\n",
    "    Linear(network_size, network_size, bias=False, generator=g, device=device), BatchNorm1d(network_size, device=device), Tanh(),\n",
    "    Linear(network_size, network_size, bias=False, generator=g, device=device), BatchNorm1d(network_size, device=device), Tanh(),\n",
    "    Linear(network_size, network_size, bias=False, generator=g, device=device), BatchNorm1d(network_size, device=device), Tanh(),\n",
    "    Linear(network_size, network_size, bias=False, generator=g, device=device), BatchNorm1d(network_size, device=device), Tanh(),\n",
    "    Linear(network_size, vocab_size, bias=True, generator=g, device=device)\n",
    "]\n",
    "\n",
    "# Layers = [\n",
    "#     Linear(context_size * embed_size, network_size, bias=True, generator=g, device=device), Tanh(),\n",
    "#     Linear(network_size, network_size, bias=True, generator=g, device=device), Tanh(),\n",
    "#     Linear(network_size, network_size, bias=True, generator=g, device=device), Tanh(),\n",
    "#     Linear(network_size, network_size, bias=True, generator=g, device=device), Tanh(),\n",
    "#     Linear(network_size, network_size, bias=True, generator=g, device=device), Tanh(),\n",
    "#     Linear(network_size, vocab_size, bias=True, generator=g, device=device),\n",
    "# ]\n",
    "\n",
    "with torch.no_grad():\n",
    "    Layers[-1].weight *= 0.1\n",
    "    for layer in Layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 1.0\n",
    "\n",
    "parameters = [E] + [p for layer in Layers for p in layer.parameters()]\n",
    "print(\"parameters: \", sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c7a4abfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50: 2.5625\n",
      "100: 2.2543\n",
      "150: 2.0946\n",
      "200: 1.9861\n",
      "250: 1.9185\n",
      "300: 1.8200\n",
      "350: 1.7953\n",
      "400: 1.7511\n",
      "450: 1.6650\n",
      "500: 1.6454\n",
      "550: 1.6765\n",
      "600: 1.5956\n",
      "650: 1.5906\n",
      "700: 1.5396\n",
      "750: 1.5166\n",
      "800: 1.5218\n",
      "850: 1.4835\n",
      "900: 1.4684\n",
      "950: 1.4415\n",
      "1000: 1.4211\n",
      "1.4210872650146484\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "X = X_train\n",
    "Y = Y_train\n",
    "lossi = []\n",
    "ud =[]\n",
    "\n",
    "for i in range(1_000):\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, X.size(0), (batch_size,), device=device)\n",
    "    X_batch, Y_batch = X[ix], Y[ix]\n",
    "\n",
    "    # forward pass\n",
    "    I = E[X_batch].view(-1, context_size * embed_size) # E -> (vocab_size, embed_size), X_batch -> (batch_size, context_size)\n",
    "\n",
    "    for layer in Layers:\n",
    "        I = layer(I)\n",
    "\n",
    "    loss = F.cross_entropy(I, Y_batch)\n",
    "\n",
    "    # backward pass\n",
    "    # for layer in Layers:\n",
    "    #     layer.out.retain_grad()  # AFTER_DEBUG: would take out retain_graph\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    if i < 1_000:\n",
    "        learning_rate = 0.01\n",
    "    elif i < 5_000:\n",
    "        learning_rate = 0.005\n",
    "    else:\n",
    "        learning_rate = 0.001\n",
    "    for p in parameters:\n",
    "        p.data += -learning_rate * p.grad\n",
    "\n",
    "    # track stats\n",
    "    lossi.append(loss.log10().item())\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"{i+1}: {loss.item():.4f}\")\n",
    "    # with torch.no_grad():\n",
    "    #     ud.append([((learning_rate * p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "    # break\n",
    "    # if i >= 1000:\n",
    "    #     break  # AFTER_DEBUG: would take out obviously to run full optimization\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "\n",
    "for i, layer in enumerate(Layers[:-1]):  # note: exclude the output layer\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out\n",
    "        print(\n",
    "            \"layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%\"\n",
    "            % (\n",
    "                i,\n",
    "                layer.__class__.__name__,\n",
    "                t.mean(),\n",
    "                t.std(),\n",
    "                (t.abs() > 0.97).float().mean() * 100,\n",
    "            )\n",
    "        )\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f\"layer {i} ({layer.__class__.__name__})\")\n",
    "plt.legend(legends)\n",
    "plt.title(\"activation distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "\n",
    "for i, layer in enumerate(Layers[:-1]):  # note: exclude the output layer\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out.grad\n",
    "        print(\n",
    "            \"layer %d (%10s): mean %+f, std %e\"\n",
    "            % (i, layer.__class__.__name__, t.mean(), t.std())\n",
    "        )\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f\"layer {i} ({layer.__class__.__name__})\")\n",
    "plt.legend(legends)\n",
    "plt.title(\"gradient distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a26392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4))  # width and height of the plot\n",
    "legends = []\n",
    "\n",
    "for i, p in enumerate(parameters):\n",
    "    t = p.grad\n",
    "    if p.ndim == 2:\n",
    "        print(\n",
    "            \"weight %10s | mean %+f | std %e | grad:data ratio %e\"\n",
    "            % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std())\n",
    "        )\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f\"{i} {tuple(p.shape)}\")\n",
    "plt.legend(legends)\n",
    "plt.title(\"weights gradient distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4937ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  if p.ndim == 2:\n",
    "    plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "    legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "plt.legend(legends);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e08db60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 1.5092628002166748\n",
      "test 1.5031850337982178\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def loss_by_split(split):\n",
    "    X, Y = {\n",
    "        \"train\": (X_train, Y_train),\n",
    "        \"val\": (X_val, Y_val),\n",
    "        \"test\": (X_test, Y_test),\n",
    "    }[split]\n",
    "    I = E[X].view(-1, context_size * embed_size)\n",
    "\n",
    "    for layer in Layers:\n",
    "        I = layer(I)\n",
    "\n",
    "    loss = F.cross_entropy(I, Y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "\n",
    "loss_by_split(\"val\")\n",
    "loss_by_split(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b9d005cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated samples: prajbeew\n",
      "Generated samples: ravinder\n",
      "Generated samples: manchamz\n",
      "Generated samples: pramhan\n",
      "Generated samples: rohan\n",
      "Generated samples: jaslamil\n",
      "Generated samples: praveen\n",
      "Generated samples: syojash\n",
      "Generated samples: shahram\n",
      "Generated samples: bhareesh\n",
      "Generated samples: kindesh\n",
      "Generated samples: haham\n",
      "Generated samples: siran\n",
      "Generated samples: anim\n",
      "Generated samples: anushar\n",
      "Generated samples: akasxhak\n",
      "Generated samples: modu\n",
      "Generated samples: deepak\n",
      "Generated samples: sunil\n",
      "Generated samples: vijay\n"
     ]
    }
   ],
   "source": [
    "# Generate samples\n",
    "def set_training_mode(layers, training=True):\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'training'):\n",
    "            layer.training = training\n",
    "set_training_mode(Layers, training=False)  \n",
    "with torch.no_grad():\n",
    "    for _ in range(20):\n",
    "        samples = []\n",
    "        context = [0] * context_size\n",
    "        while True:\n",
    "            I = E[torch.tensor([context])].view(-1, context_size * embed_size)\n",
    "            for layer in Layers:\n",
    "                I = layer(I)\n",
    "            logits = I\n",
    "            # print(logits)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx = torch.multinomial(probs, num_samples=1).item()\n",
    "            if idx == 0:\n",
    "                break\n",
    "            samples.append(idx)\n",
    "            context = context[1:] + [idx]\n",
    "\n",
    "        print(\"Generated samples:\", \"\".join(idx_to_token[i] for i in samples))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
