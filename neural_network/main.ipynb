{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2f11801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\"stores a single scalar value and its gradient\"\"\"\n",
    "\n",
    "    def __init__(self, data, _previous=(), _operation=\"\"):\n",
    "        self.data = data\n",
    "        self.gradient = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._previous = set(_previous)\n",
    "        self._operation = (\n",
    "            _operation  # the op that produced this node, for graphviz / debugging / etc\n",
    "        )\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.gradient += out.gradient\n",
    "            other.gradient += out.gradient\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.gradient += other.data * out.gradient\n",
    "            other.gradient += self.data * out.gradient\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(\n",
    "            other, (int, float)\n",
    "        ), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f\"**{other}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.gradient += (other * self.data ** (other - 1)) * out.gradient\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), \"ReLU\")\n",
    "\n",
    "        def _backward():\n",
    "            self.gradient += (out.data > 0) * out.gradient\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(current):\n",
    "            if current not in visited:\n",
    "                visited.add(current)\n",
    "                for child in current._previous:\n",
    "                    build_topo(child)\n",
    "                topo.append(current)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.gradient = 1\n",
    "        for current in reversed(topo):\n",
    "            current._backward()\n",
    "\n",
    "    def __neg__(self):  # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other):  # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other):  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):  # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):  # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other):  # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.gradient})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "8dc254e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Module:\n",
    "    \"\"\"Base class for all modules in the neural network.\"\"\"\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for parameter in self.parameters():\n",
    "            parameter.gradient = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "class Neuron(Module):\n",
    "    \"\"\"Represents a single neuron in the neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, nin, nonlinear=True, neuron_name=None):\n",
    "        self.weights = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.bias = Value(0)\n",
    "        self.nonlinear = nonlinear\n",
    "        self.neuron_name = neuron_name\n",
    "        self.activation = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        activation = sum((wi * xi for wi, xi in zip(self.weights, x)), self.bias)\n",
    "        self.activation = activation.relu() if self.nonlinear else activation\n",
    "        return self.activation\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.weights + [self.bias]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.neuron_name}\"\n",
    "\n",
    "\n",
    "class Layer(Module):\n",
    "    \"\"\"Represents a layer of neurons in the neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, nin, nout, nonlinear, layer_name):\n",
    "        self.layer_name = layer_name\n",
    "        self.neurons = [\n",
    "            Neuron(nin, nonlinear, f\"{layer_name}_neuron_{i}\") for i in range(nout)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [\n",
    "            parameter for neuron in self.neurons for parameter in neuron.parameters()\n",
    "        ]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"[{'\\n'.join(str(neuron) for neuron in self.neurons)}]\"\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    \"\"\"Represents a multi-layer perceptron (MLP) neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, nin, nouts, layer_names):\n",
    "        size = [nin] + nouts\n",
    "        self.layers = [\n",
    "            Layer(size[i], size[i + 1], i != len(nouts) - 1, layer_names[i])\n",
    "            for i in range(len(nouts))\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [parameter for layer in self.layers for parameter in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"[{'\\n\\n'.join(f'{layer.layer_name} => {layer}' for layer in self.layers)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "3fb01506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, time\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NeuronSnapshot:\n",
    "    layer: str\n",
    "    neuron: str\n",
    "    activation: str\n",
    "    weights: List[float]\n",
    "    bias: float\n",
    "\n",
    "\n",
    "def snapshot_mlp(mlp) -> Dict[str, Any]:\n",
    "    \"\"\"Hierarchical snapshot: layers → neurons → weights/bias.\"\"\"\n",
    "\n",
    "    layers = []\n",
    "    for layer in mlp.layers:\n",
    "        neurons = []\n",
    "        for neuron in layer.neurons:\n",
    "            neurons.append(\n",
    "                asdict(\n",
    "                    NeuronSnapshot(\n",
    "                        layer=layer.layer_name,\n",
    "                        neuron=neuron.neuron_name,\n",
    "                        activation=str(neuron.activation.data) if neuron.activation else \"None\",\n",
    "                        weights=[float(weight.data) for weight in neuron.weights],\n",
    "                        bias=float(neuron.bias.data),\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        layers.append({\"layer\": layer.layer_name, \"neurons\": neurons})\n",
    "    return {\"layers\": layers}\n",
    "\n",
    "\n",
    "def write_snapshot(\n",
    "    payload: Dict[str, Any], step: int, dirpath=\"snapshots\", keep_history=False\n",
    "):\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    payload = {\"step\": step, \"timestamp\": time.time(), \"snapshot\": payload}\n",
    "\n",
    "    with open(os.path.join(dirpath, \"latest.json\"), \"w\") as f:\n",
    "        json.dump(payload, f)\n",
    "\n",
    "    if keep_history:\n",
    "        with open(os.path.join(dirpath, f\"step_{step:06d}.json\"), \"w\") as f:\n",
    "            json.dump(payload, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e32fb8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50  mean_squared_error=10.0813\n",
      "Epoch 2/50  mean_squared_error=9.5022\n",
      "Epoch 3/50  mean_squared_error=9.0247\n",
      "Epoch 4/50  mean_squared_error=8.6429\n",
      "Epoch 5/50  mean_squared_error=8.3252\n",
      "Epoch 6/50  mean_squared_error=8.0521\n",
      "Epoch 7/50  mean_squared_error=7.8489\n",
      "Epoch 8/50  mean_squared_error=7.6556\n",
      "Epoch 9/50  mean_squared_error=7.4699\n",
      "Epoch 10/50  mean_squared_error=7.2916\n",
      "Epoch 11/50  mean_squared_error=7.1203\n",
      "Epoch 12/50  mean_squared_error=6.9558\n",
      "Epoch 13/50  mean_squared_error=6.7978\n",
      "Epoch 14/50  mean_squared_error=6.6461\n",
      "Epoch 15/50  mean_squared_error=6.5004\n",
      "Epoch 16/50  mean_squared_error=6.3605\n",
      "Epoch 17/50  mean_squared_error=6.2261\n",
      "Epoch 18/50  mean_squared_error=6.0970\n",
      "Epoch 19/50  mean_squared_error=5.9731\n",
      "Epoch 20/50  mean_squared_error=5.8540\n",
      "Epoch 21/50  mean_squared_error=5.7397\n",
      "Epoch 22/50  mean_squared_error=5.6299\n",
      "Epoch 23/50  mean_squared_error=5.5244\n",
      "Epoch 24/50  mean_squared_error=5.4231\n",
      "Epoch 25/50  mean_squared_error=5.3258\n",
      "Epoch 26/50  mean_squared_error=5.2324\n",
      "Epoch 27/50  mean_squared_error=5.1427\n",
      "Epoch 28/50  mean_squared_error=5.0565\n",
      "Epoch 29/50  mean_squared_error=4.9738\n",
      "Epoch 30/50  mean_squared_error=4.8943\n",
      "Epoch 31/50  mean_squared_error=4.8179\n",
      "Epoch 32/50  mean_squared_error=4.7446\n",
      "Epoch 33/50  mean_squared_error=4.6742\n",
      "Epoch 34/50  mean_squared_error=4.6066\n",
      "Epoch 35/50  mean_squared_error=4.5417\n",
      "Epoch 36/50  mean_squared_error=4.4793\n",
      "Epoch 37/50  mean_squared_error=4.4194\n",
      "Epoch 38/50  mean_squared_error=4.3619\n",
      "Epoch 39/50  mean_squared_error=4.3066\n",
      "Epoch 40/50  mean_squared_error=4.2536\n",
      "Epoch 41/50  mean_squared_error=4.2026\n",
      "Epoch 42/50  mean_squared_error=4.1537\n",
      "Epoch 43/50  mean_squared_error=4.1066\n",
      "Epoch 44/50  mean_squared_error=4.0615\n",
      "Epoch 45/50  mean_squared_error=4.0181\n",
      "Epoch 46/50  mean_squared_error=3.9765\n",
      "Epoch 47/50  mean_squared_error=3.9365\n",
      "Epoch 48/50  mean_squared_error=3.8981\n",
      "Epoch 49/50  mean_squared_error=3.8612\n",
      "Epoch 50/50  mean_squared_error=3.8258\n"
     ]
    }
   ],
   "source": [
    "def f(x1, x2):\n",
    "    return x1 * x1 + 0.5 * x2\n",
    "\n",
    "\n",
    "train_X = [\n",
    "    [-2, 0],\n",
    "    [-2, 1],\n",
    "    [-2, 2],\n",
    "    [-1, 0],\n",
    "    [-1, 1],\n",
    "    [-1, 2],\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [0, 2],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [1, 2],\n",
    "    [2, 0],\n",
    "    [2, 1],\n",
    "    [2, 2],\n",
    "]\n",
    "train_Y = [f(x1, x2) for (x1, x2) in train_X]\n",
    "\n",
    "val_X = [[-1.5, 0.5], [0.5, 1.5], [1.5, 0.5], [-0.5, 2.0]]\n",
    "val_Y = [f(x1, x2) for (x1, x2) in val_X]\n",
    "\n",
    "\n",
    "def mean_square_error(predictions, targets):\n",
    "    differences = [\n",
    "        (prediction - target) for prediction, target in zip(predictions, targets)\n",
    "    ]\n",
    "    squared_differences = [difference * difference for difference in differences]\n",
    "\n",
    "    return sum(squared_differences) * (1.0 / len(squared_differences))\n",
    "\n",
    "\n",
    "def tune_parameters(parameters, learning_rate):\n",
    "    for parameter in parameters:\n",
    "        parameter.data += -learning_rate * parameter.gradient\n",
    "\n",
    "\n",
    "def train(mlp, train_X, train_Y, epochs=10, learning_rate=0.1):\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        predictions = [mlp([Value(x1), Value(x2)]) for x1, x2 in train_X]\n",
    "        loss = mean_square_error(predictions, [Value(y) for y in train_Y])\n",
    "        print(f\"Epoch {epoch+1}/{epochs}  mean_squared_error={loss.data:.4f}\")\n",
    "\n",
    "        # Backward pass\n",
    "        mlp.zero_grad()\n",
    "        loss.backward()\n",
    "        tune_parameters(mlp.parameters(), learning_rate)\n",
    "\n",
    "        # Snapshot\n",
    "        snapshot = snapshot_mlp(mlp)\n",
    "        write_snapshot(snapshot, epoch + 1)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "mlp = MLP(2, [4, 3, 4, 1], [\"hidden1\", \"hidden2\", \"hidden3\", \"logits\"])\n",
    "train(mlp, train_X, train_Y, epochs=50, learning_rate=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
